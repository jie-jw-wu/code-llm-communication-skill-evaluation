# very random notes

run dos2unix XXX.sh before running sbatch XXX.sh in Sockeye

module load CVMFS_CC python/3.8.10 gcc/11.3.0 py-virtualenv/16.7.6
pip install nltk transformers openai accelerate
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
salloc  --account=st-fhendija-1-gpu --partition=interactive_gpu --time=1:0:0 -N 1 -n 2 --mem=8G --gpus=1

not available:
pip install -U pip
no gpu being used (using cuda.available())  

TODO: install conda

https://confluence.it.ubc.ca/display/UARC/Using+Virtual+Environments+on+Sockeye#UsingVirtualEnvironmentsonSockeye-Creatingandusingacondaenvironment


module load gcc/9.4.0 python/3.8.10 py-virtualenv/16.7.6
source env_jw_py3.8.10/bin/activate


Currently Loaded Modules:
  1) shared           3) gmp/6.2.1   5) ncurses/6.2     7) py-setuptools/50.3.2   9) miniconda3/4.9.2
  2) DefaultModules   4) gcc/9.4.0   6) python/3.8.10   8) py-virtualenv/16.7.6


# for sockeye:  
python generate_response.py -d HumanEvalComm -m starcoderbase-1b -n 1 -t 1 -s 0 -o manualRemove --hf_dir /scratch/st-fhendija-1/jwu/cache --model_name_or_path bigcode/starcoderbase-1b -maxp -1
save model: 
python generate_response.py -d HumanEvalComm -m starcoderbase-1b -n 1 -t 1 -s 0 -o manualRemove --hf_dir /scratch/st-fhendija-1/jwu/cache --model_name_or_path /scratch/st-fhendija-1/jwu/bigcode/starcoderbase-1b -maxp -1 --do_save_model


create folder(s): /scratch/st-fhendija-1/jwu/bigcode/starcoderbase-1b


go into interactive gpu node:
 module load gcc python miniconda3 cuda cudnn git (see https://confluence.it.ubc.ca/display/UARC/TensorFlow+with+Conda. this is essential)
    (base) [jwu153@se061 ~]$ python
    Python 3.8.5 (default, Sep  4 2020, 07:30:14)
    [GCC 7.3.0] :: Anaconda, Inc. on linux
    Type "help", "copyright", "credits" or "license" for more information.
    >>> import torch
    >>> print(torch.cuda.is_available())
    True

activate conda env: 
    conda activate /arc/project/st-fhendija-1/jwu/jw-gpu



STARCODER:
python generate_response.py -d HumanEvalComm -m starcoderbase-7b -n 1 -t 1 -s 0 -o manualRemove --hf_dir /scratch/st-fhendija-1/jwu/cache --model_name_or_path /arc/project/st-fhendija-1/jwu/bigcode/starcoderbase-7b -maxp -1 --seq_length 1000000000
python generate_response.py -d HumanEvalComm -m starcoderbase-7b -n 1 -t 1 -s 0 -o manualRemove --hf_dir /scratch/st-fhendija-1/jwu/cache --model_name_or_path bigcode/starcoderbase-7b --saved_model_path /scratch/st-fhendija-1/jwu/bigcode/starcoderbase-7b -maxp -1 --do_save_model

CODELLAMA:
python generate_response.py -d HumanEvalComm -m CodeLlama-7b-hf -n 1 -t 1 -s 0 -o manualRemove --hf_dir /scratch/st-fhendija-1/jwu/cache --model_name_or_path /arc/project/st-fhendija-1/jwu/codellama/CodeLlama-7b-hf -maxp -1 --seq_length 1000000000

NOTE: should put data in /project instead of /scratch!

run:
python generate_response.py -d HumanEvalComm -m CodeLlama-7b-hf -n 1 -t 1 -s 0 -o manualRemove --hf_dir /scratch/st-fhendija-1/jwu/cache --model_name_or_path codellama/CodeLlama-7b-hf --saved_model_path /arc/project/st-fhendija-1/jwu/codellama/CodeLlama-7b-hf -maxp -1 --do_save_model \
python generate_response.py -d HumanEvalComm -m CodeLlama-13b-hf -n 1 -t 1 -s 0 -o manualRemove --hf_dir /scratch/st-fhendija-1/jwu/cache --model_name_or_path codellama/CodeLlama-13b-hf --saved_model_path /arc/project/st-fhendija-1/jwu/codellama/CodeLlama-13b-hf -maxp -1 --do_save_model \
python generate_response.py -d HumanEvalComm -m CodeLlama-34b-hf -n 1 -t 1 -s 0 -o manualRemove --hf_dir /scratch/st-fhendija-1/jwu/cache --model_name_or_path codellama/CodeLlama-34b-hf --saved_model_path /arc/project/st-fhendija-1/jwu/codellama/CodeLlama-34b-hf -maxp -1 --do_save_model \
python generate_response.py -d HumanEvalComm -m CodeLlama-7b-Instruct-hf -n 1 -t 1 -s 0 -o manualRemove --hf_dir /scratch/st-fhendija-1/jwu/cache --model_name_or_path codellama/CodeLlama-7b-Instruct-hf --saved_model_path /arc/project/st-fhendija-1/jwu/codellama/CodeLlama-7b-Instruct-hf -maxp -1 --do_save_model \
python generate_response.py -d HumanEvalComm -m CodeLlama-13b-Instruct-hf -n 1 -t 1 -s 0 -o manualRemove --hf_dir /scratch/st-fhendija-1/jwu/cache --model_name_or_path codellama/CodeLlama-13b-Instruct-hf --saved_model_path /arc/project/st-fhendija-1/jwu/codellama/CodeLlama-13b-Instruct-hf -maxp -1 --do_save_model \
python generate_response.py -d HumanEvalComm -m CodeLlama-34b-Instruct-hf -n 1 -t 1 -s 0 -o manualRemove --hf_dir /scratch/st-fhendija-1/jwu/cache --model_name_or_path codellama/CodeLlama-34b-Instruct-hf --saved_model_path /arc/project/st-fhendija-1/jwu/codellama/CodeLlama-34b-Instruct-hf -maxp -1 --do_save_model \
python generate_response.py -d HumanEvalComm -m starcoderbase-3b -n 1 -t 1 -s 0 -o manualRemove --hf_dir /scratch/st-fhendija-1/jwu/cache --model_name_or_path bigcode/starcoderbase-3b --saved_model_path /arc/project/st-fhendija-1/jwu/bigcode/starcoderbase-3b -maxp -1 --do_save_model \


multi tasking using `screen`:
https://askubuntu.com/questions/332104/open-another-terminal-window-with-the-same-ssh-session-as-original-window


starcoder  starcoderbase  starcoderbase-1b  starcoderbase-3b  starcoderbase-7b  starcoderplus

3/18

try using 8int version to speedup:

python generate_response.py -d HumanEvalComm -m CodeLlama-7b-hf -n 1 -t 1 -s 0 -o manualRemove --hf_dir /scratch/st-fhendija-1/jwu/cache --model_name_or_path /arc/project/st-fhendija-1/jwu/codellama/CodeLlama-7b-hf -maxp -1 --seq_length 1000000000 --use_int8


testing:
python generate_response.py -d HumanEvalComm -m CodeLlama-7b-hf -n 1 -t 1 -s 0 -o manualRemove --hf_dir /scratch/st-fhendija-1/jwu/cache --model_name_or_path /arc/project/st-fhendija-1/jwu/codellama/CodeLlama-7b-hf -maxp -1 --seq_length 1000000000 --do_test_only --user_input="You are an expert software developer who writes high quality code. With below information, please either generate Python3 code (Respond directly with code only with markdown), or ask clarifying questions: \nfrom typing import List\ndef candidate(...) -> bool:\n \"\"\" Check given a list of number.\"\"\"\n"
python generate_response.py -d HumanEvalComm -m CodeLlama-7b-hf -n 1 -t 1 -s 0 -o manualRemove --hf_dir /scratch/st-fhendija-1/jwu/cache --model_name_or_path /arc/project/st-fhendija-1/jwu/codellama/CodeLlama-7b-hf -maxp -1 --seq_length 20 --do_test_only --user_input="def string_length(s):"


check disk quota:
du -h --max-depth=1 /home/jwu153

an issue: i moved miniconda3 from /home/jwu153 to /arc/project/.../jwu/miniconda3, but the path is messed up completely. Fortunately, I was able to 'module load miniconda3' and do the rest jobs about conda. So I'll leave it as is now.

4/25
test_gpu_phase1.sh deepseek-coder-6.7b-instruct deepseek-ai/deepseek-coder-6.7b-instruct
test_gpu_phase1.sh deepseek-llm-7b-chat deepseek-ai/deepseek-llm-7b-chat
test_gpu_phase1.sh CodeQwen1.5-7B-Chat Qwen/CodeQwen1.5-7B-Chat
test_gpu_phase1.sh Meta-Llama-3-8B-Instruct meta-llama/Meta-Llama-3-8B-Instruct
test_gpu_phase1.sh deepseek-coder-33b-instruct deepseek-ai/deepseek-coder-33b-instruct